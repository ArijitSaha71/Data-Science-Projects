E - Step

    # p = mixture.p
    # mu = mixture.mu
    # var = mixture.var
    # n = X.shape[0]
    # K = mu.shape[0]
    
    # # rows, cols = np.where(X == 0)
    # # zero_indices = np.ndarray(shape=(len(rows),), dtype = object)

    # # for i in range(len(rows)):
    # #   zero_indices[i] = rows[i], cols[i]
        
    
    
    # # likelihood of each data point from each cluster, dimension n,k
    # Gaussian_pdf_matrix = np.ndarray(shape=(n, K))
    # #posterior liklihood, dimension n,k
    # post = np.ndarray(shape=(n, K))
    # log_likelihood_of_data = 0

    # for i in range(n):
    #   for j in range(K):
    #       len_C_i = len(np.delete(X[i], np.where(X[i] == 0)))
    #       mu_j = np.where(X[i] == 0, 0, mu[j]) 
    #       Gaussian_pdf_matrix[i][j] = (1/(2*np.pi*var[j]))**(len_C_i/2)*np.exp(-(np.linalg.norm(X[i]-mu_j)**2)/(2*var[j]))
    #       post[i, j] = np.log(p[j]) + np.log(Gaussian_pdf_matrix[i, j])
    #   total = logsumexp(post[i])
    #   post[i] = np.exp(post[i] - total)
    #   log_likelihood_of_data += np.sum(post[i]*total)
    
    # return post,log_likelihood_of_data


M - Step

    # n = X.shape[0]
    # K = post.shape[1]
    # d = X.shape[1]
    # mu = mixture.mu
    # mu_nominator = np.zeros((K, d))
    # var_nominator = np.zeros((K, ))
    # new_mu = np.ndarray(shape=(K, d))
    # new_var = np.ndarray(shape=(K,))
    
    # sum_ = np.sum(post, axis=0)
    
    # new_p = sum_/n
    # mu_nominator = np.transpose(post) @ X
    
    # for j in range(K):
    #     for i in range(n):
    #         if sum_[j] >= 1: 
    #            new_mu[j] = np.where(X[i] == 0, 0, mu_nominator[j]/sum_[j])
    #         else:
    #             new_mu[j] = mu[j]
    #             #var_nominator[j] += post[i][j]*(np.linalg.norm(X[i]-new_mu[j])**2)
    #             new_var[j] += post[i][j]*(np.linalg.norm(X[i]-new_mu[j])**2)/(len(np.delete(X[i], np.where(X[i] == 0)))*post[i][j])
    #     #new_var[j] = var_nominator[j]/(len(np.delete(X[i], np.where(X[i] == 0)))*sum)
    # #new_var = var_nominator/(d*sum)
    # for j in range(K):
    #     for l in range(d):
    #         for i in range(n):
    #            if X[i][l] == 0:
    #               new_mu[j][l] += 0
    #            else:
    #                if sum_[j] >= 1:
    #                   new_mu[j][l] += post[i][j]*X[i][l]/(post[i][j])
    # return GaussianMixture(new_mu, new_var, new_p)